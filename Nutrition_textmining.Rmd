---
title: "Nutrition_textmining"
author: "Mariateresa Russo"
date: "2025-05-19"
output: html_document
---
PULIZIA INIZIALE
```{r}
library(readr)
library(dplyr)
library(tidyr)
library(purrr)
library(combinat)
library(dplyr)
library(stringr)
library(tm)
library(ggplot2)

nutrition_comments = read.csv("nutrition_comments2.csv")
nutrition_threads = read.csv("nutrition_threads2.csv")

nutrition_threads
nutrition_comments 

sum(nutrition_threads$comments)
nutrition_comments %>%
  filter(!grepl("_", comment_id)) %>%
  summarise(total_main_comments = n())

nutrition_comments <- nutrition_comments %>%
  mutate(thread = as.integer(str_extract(comment_id, "^[0-9]+")))

table(nutrition_threads$comments)
table(as.numeric(nutrition_comments$thread))
```

```{r}
remove_function <-content_transformer(function(x, pattern) gsub(pattern,' ',x))

remove_urls <- function(text) {
  text <- gsub("(f|ht)tp(s?)://[^ ]+", "", text) #remove url and links
  text <- gsub("bit\\.ly/\\w+", "", text)
  return(text)
}

remove_short_words <- function(text) { #rimuove parole con meno di 2 lettere
  words <- unlist(strsplit(text, "\\s+"))
  long_words <- words[nchar(words) > 2]
  cleaned_text <- paste(long_words, collapse = " ")
  return(cleaned_text)
}

toUTF8 <- content_transformer(function(x) iconv(x, from = "", to = "UTF-8", sub = ""))
```


Pulizia threads
```{r}
corpus <- VCorpus(VectorSource(nutrition_threads$text))
corpus <- tm_map(corpus, toUTF8)
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, content_transformer(remove_urls))
corpus <- tm_map(corpus, remove_function, "/")
corpus <- tm_map(corpus, remove_function, "'")
corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, removeWords, stopwords("english"))
corpus <- tm_map(corpus, content_transformer(remove_short_words))
corpus <- tm_map(corpus, stripWhitespace)

custom_tokens <- c(
  "t", "wasn", "didn", "haven", "mnm", "don", "ve", "ain", "t", "won", "doesn", "let", "s",
  "doesnt", "cant", "dont", "etc", "hey", "whatnot", "thisclassic", "joe", "couldn", "re",
  "isnt", "non", "issn", "aren", "arent", "ll", "isn", "s", "metrx", "yrs", "cvs", "alr",
  "chestgtbackgtlegs", "kgcompared", "btw", "tbsp", "irb", "amam", "alot", "didnt", "shouldn",
  "somethingi", "chia", "hadn", "esn", "alllll", "idk", "can", "however", "probably",
  "without", "now", "would", "get", "wouldn"
)

corpus <- tm_map(corpus, removeWords, custom_tokens)

corpus_content <- sapply(corpus, as.character)
df_new_threads <- data.frame(Corpus = corpus_content)
```

Pulizia comments
```{r}
corpus2 <- VCorpus(VectorSource(nutrition_comments$comment))
corpus2 <- tm_map(corpus2, toUTF8)
corpus2 <- tm_map(corpus2, content_transformer(tolower))
corpus2 <- tm_map(corpus2, content_transformer(remove_urls))
corpus2 <- tm_map(corpus2, remove_function, "/")
corpus2 <- tm_map(corpus2, remove_function, "'")
corpus2 <- tm_map(corpus2, removePunctuation)
corpus2 <- tm_map(corpus2, removeNumbers)
corpus2 <- tm_map(corpus2, removeWords, stopwords("english"))
corpus2 <- tm_map(corpus2, content_transformer(remove_short_words))
corpus2 <- tm_map(corpus2, stripWhitespace)

corpus2 <- tm_map(corpus2, removeWords, custom_tokens)

corpus_content2 <- sapply(corpus2, as.character)
df_new_comments <- data.frame(Corpus = corpus_content2)
```

Salvataggio in csv
```{r}
# Rimuovi la colonna 'text' e aggiungi il testo pulito
df_new_threads <- nutrition_threads %>%
  select(-text) %>%
  mutate(Cleaned_Text = df_new_threads$Corpus)

# Rimuovi la colonna 'comment' e aggiungi il commento pulito
df_new_comments <- nutrition_comments %>%
  select(-comment) %>%
  mutate(Cleaned_Comment = df_new_comments$Corpus)

# Salvataggio in CSV
write.csv(df_new_threads, "nutrition_threads_cleaned.csv", row.names = FALSE)
write.csv(df_new_comments, "nutrition_comments_cleaned.csv", row.names = FALSE)

```

THREADS 

Term document matrix
```{r}
suppressWarnings({
#tdm without stemming
tdm_1x <- DocumentTermMatrix(corpus)
print('Term-document matrix before stemming: ')
print(tdm_1x) 

#tf
corpus <- tm_map(corpus, stemDocument, language = 'english')
tdm_1 <- DocumentTermMatrix(corpus)
print('Term-document matrix after stemming: ')
print(tdm_1)
})

"
#tf-idf
suppressWarnings({
  tdm_1 <- DocumentTermMatrix(corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
  print(tdm_1)
  
  corpus <- tm_map(corpus, stemDocument, language = 'english')
})
"
```
```{r}
tdm_df_threads <- as.data.frame(as.matrix(tdm_1))
#tdm_df
```

rimozione parole rare
```{r}
#column frequencies
column_frequencies <- colSums(tdm_df_threads)
frequency_df <- data.frame(t(column_frequencies))
print(frequency_df)

#common words and rare words
frequency_df_transposed <- t(frequency_df)
frequency_tbl <- as_tibble(frequency_df_transposed)

quantile(frequency_tbl$V1, probs = 0.5) 
quantile(frequency_tbl$V1, probs = 0.6) 
quantile(frequency_tbl$V1, probs = 0.7)
quantile(frequency_tbl$V1, probs = 0.8)
quantile(frequency_tbl$V1, probs = 0.9)
quantile(frequency_tbl$V1, probs = 0.95)
quantile(frequency_tbl$V1, probs = 0.96)
quantile(frequency_tbl$V1, probs = 0.97)
quantile(frequency_tbl$V1, probs = 0.98)
quantile(frequency_tbl$V1, probs = 0.99)
quantile(frequency_tbl$V1, probs = 1)

word_freq_under6 <- names(column_frequencies)[column_frequencies < 30] # tolgo tutte le parole sotto la frequenza 3
rare_words_list <- as.list(word_freq_under6 )

tdm_df_threads <- tdm_df_threads[, !colnames(tdm_df_threads) %in% rare_words_list]

column_frequencies <- colSums(tdm_df_threads)
frequency_df <- data.frame(t(column_frequencies))
frequenze <- as.numeric(frequency_df[1, ])
ordered_indices <- order(frequenze, decreasing = TRUE)
ordered_frequency_df <- frequency_df[, ordered_indices]
ordered_frequency_df 
```

```{r}
final_df_threads <- tdm_df_threads
#final_df <- subset(tdm_df_threads, select = -c(didn, don, didn, doesn, can, don,etc, hey))
#final_df<-final_df[ , -20]
```

#codice vecchio per matrice term-term threads

library(text2vec)

corpus_char <- sapply(corpus, content)

# 1. Tokenizzazione del corpus
tokens_thr <- itoken(corpus_char, tokenizer = word_tokenizer, progressbar = FALSE)

# 2. Ottieni il vocabolario delle parole in final_df
# Supponiamo che le colonne di final_df siano i termini
vocab_terms_thr <- colnames(final_df_threads)

# 3. Crea il vocabolario solo con questi termini
vocab_thr <- create_vocabulary(tokens_thr)
#vocab_thr <- prune_vocabulary(vocab_thr, term_count_min = 10)
vocab_thr <- vocab_thr[vocab_thr$term %in% vocab_terms_thr, ]

# 4. Crea il vettorizzatore con il vocabolario filtrato
vectorizer_thr <- vocab_vectorizer(vocab_thr)

# 5. Crea la matrice di co-occorrenza (Term-Term Matrix)
term_term_thr <- create_tcm(tokens_thr,
                            vectorizer_thr,
                            skip_grams_window = 3,
                            skip_grams_window_context = "right")




COMMENTI

Term document matrix
```{r}
suppressWarnings({
#tdm without stemming
tdm_2x <- DocumentTermMatrix(corpus2)
print('Term-document matrix before stemming: ')
print(tdm_2x) 

#tf
corpus2 <- tm_map(corpus2, stemDocument, language = 'english')
tdm_2 <- DocumentTermMatrix(corpus2)
print('Term-document matrix after stemming: ')
print(tdm_2)
})

"
#tf-idf
suppressWarnings({
  tdm_2 <- DocumentTermMatrix(corpus, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE)))
  print(tdm_2)
  
  corpus <- tm_map(corpus, stemDocument, language = 'english')
})
"
```


```{r}
tdm_df_comments <- as.data.frame(as.matrix(tdm_2))
#tdm_df
```

rimozione parole rare
```{r}
#column frequencies
column_frequencies2 <- colSums(tdm_df_comments)
frequency_df2 <- data.frame(t(column_frequencies2))
print(frequency_df2)

#common words and rare words
frequency_df_transposed2 <- t(frequency_df2)
frequency_tbl2 <- as_tibble(frequency_df_transposed2)

quantile(frequency_tbl2$V1, probs = 0.5) 
quantile(frequency_tbl2$V1, probs = 0.6) 
quantile(frequency_tbl2$V1, probs = 0.7)
quantile(frequency_tbl2$V1, probs = 0.8)
quantile(frequency_tbl2$V1, probs = 0.9)
quantile(frequency_tbl2$V1, probs = 0.95)
quantile(frequency_tbl2$V1, probs = 0.96)
quantile(frequency_tbl2$V1, probs = 0.97)
quantile(frequency_tbl2$V1, probs = 0.98)
quantile(frequency_tbl2$V1, probs = 0.99)
quantile(frequency_tbl2$V1, probs = 1)

word_freq_under3_2 <- names(column_frequencies2)[column_frequencies2 < 10] # tolgo tutte le parole sotto la frequenza 3
rare_words_list2 <- as.list(word_freq_under3_2 )

tdm_df_comments <- tdm_df_comments[, !colnames(tdm_df_comments) %in% rare_words_list2]

column_frequencies2 <- colSums(tdm_df_comments)
frequency_df2 <- data.frame(t(column_frequencies2))
frequenze2 <- as.numeric(frequency_df2[1, ])
ordered_indices2 <- order(frequenze2, decreasing = TRUE)
ordered_frequency_df2 <- frequency_df2[, ordered_indices2]
ordered_frequency_df2 
```

```{r}
final_df_comments = tdm_df_comments
#final_df2 <- subset(tdm_df_comments, select = -c(didn, don, didn, doesn, can, don,etc, hey))
final_df_comments<-final_df_comments[ , -1]
```

costruzione comment comment con window size

corpus2_char <- sapply(corpus2, content)

# 1. Tokenizzazione del corpus
tokens_com <- itoken(corpus2_char, tokenizer = word_tokenizer, progressbar = FALSE)

# 2. Ottieni il vocabolario delle parole in final_df
# Supponiamo che le colonne di final_df siano i termini
vocab_terms_com <- colnames(final_df_comments)

# 3. Crea il vocabolario solo con questi termini
vocab_com <- create_vocabulary(tokens_com)
vocab_com <- prune_vocabulary(vocab_thr, term_count_min = 20)
vocab_com<- vocab_com[vocab_com$term %in% vocab_terms_com, ]

# 4. Crea il vettorizzatore con il vocabolario filtrato
vectorizer_com <- vocab_vectorizer(vocab_com)


weight_com = c(1, 0.5, 0, 0)

# 5. Crea la matrice di co-occorrenza (Term-Term Matrix)
term_term_com <- create_tcm(tokens_com,
                            vectorizer_com,
                            skip_grams_window = 4,
                            skip_grams_window_context = "right",
                            weights = weight_com)


DATAFRAMES PER FARE LE RETI ONE MODE PAROLE-PAROLE SU GEPHI

parole-parole nei threads
```{r}
rownames(final_df_threads) <- NULL
final_df_threads 

all_edges_threads <- list()
for (i in 1:nrow(final_df_threads)) {
  words_in_doc_t <- names(final_df_threads)[final_df_threads[i, ] == 1]
  
  # Se almeno due parole presenti, crea combinazioni a coppie
  if (length(words_in_doc_t) >= 2) {
    pairs <- combn(words_in_doc_t, 2, simplify = FALSE)
    all_edges_threads <- append(all_edges_threads, pairs)
  }
}

# Trasforma le combinazioni in un dataframe Source-Target
edges_df_threads <- do.call(rbind, lapply(all_edges_threads, function(x) data.frame(Source = x[1], Target = x[2])))

# Grafo non direzionato
edges_df_threads <- distinct(edges_df_threads)

edges_df_threads

write.csv(edges_df_threads, "edges_df_threads.csv", row.names = FALSE)
```

parole-parole nei comments
```{r}
rownames(final_df_comments) <- NULL
final_df_comments 

all_edges_comments <- list()
for (i in 1:nrow(final_df_comments)) {
  words_in_doc_c <- names(final_df_comments)[final_df_comments[i, ] == 1]
  
  # Se almeno due parole presenti, crea combinazioni a coppie
  if (length(words_in_doc_c) >= 2) {
    pairsc <- combn(words_in_doc_c, 2, simplify = FALSE)
    all_edges_comments <- append(all_edges_comments, pairsc)
  }
}

# Trasforma le combinazioni in un dataframe Source-Target
edges_df_comments <- do.call(rbind, lapply(all_edges_comments, function(x) data.frame(Source = x[1], Target = x[2])))

# Grafo non direzionato
edges_df_comments <- distinct(edges_df_comments)

edges_df_comments

write.csv(edges_df_comments, "edges_df_comments.csv", row.names = FALSE)
```




DATAFRAMES UNITI
```{r}

df_new_comments = read.csv("nutrition_comments_cleaned.csv")
df_new_threads = read.csv("nutrition_threads_cleaned.csv")

combined_df <- data.frame(Cleaned_Text = c(df_new_threads$Cleaned_text, df_new_comments$Cleaned_Comment))
```

